{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction from trace\n",
    "This notebook aims to take the `.pcap` files generated from the trace and use those to create a csv file that contains the features and labels to input to our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "from typing import Any, List, Literal, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas.core.frame import DataFrame\n",
    "from scapy.all import *\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client-Server interaction - which data should be kept\n",
    "We first recall the implementation of client server interaction. Indeed, this one is described in `handout_project_secretstroll.pdf`. Taking a look at implementation of `server.py` and `client.py`, we must make key observations:\n",
    "### PoI answers - size and shuffling\n",
    "* When querying the server, the client will sequentially:\n",
    "  * retrieve the list of PoI (**which can pretty much vary in size between grids**)\n",
    "  * the client then iterates through each PoI -> **no shuffling** is performed at any step, this lets us suppose that 2 queries for the same grid will produce a **same sequence of queries for the PoI infos** to the server.\n",
    "  * they make a query to retrieve the PoI infos for each. We must note that the **number of PoI ratings may vary a lot from a PoI to another**, making a PoI info response trace pretty unique.\n",
    "* When receiving client's queries, the server will:\n",
    "  * return the list of around PoI (as explained with a probably preserved order between queries). Here comes a pretty nasty trick: the **implementation given seems to not take into account the variety of possible queries**. Indeed, independently of the queries `types` of PoI (the user subscription), the server will in any case answer by **giving the full list of PoI in the grid** (`records = PoI.query.filter_by(grid_id=grid_id).all()`). This should not happen under the functionnalities described in the handout, but we'll make an adapted training set that will not need to test different types of queries, only the grid ID will vary. If we were to make the attack under fully functionnal system, we'd make a way bigger training set considering the different subscription traces and their combinations.\n",
    "  * when queried on the info for the point of interest, makes an answer that contains all its information. Note that the **number of ratings associated to a PoI has the most significant impact on the trace**.\n",
    "\n",
    "### Interaction through tor network\n",
    "Finally, a last point that should be mentionned is the **usage of tor**:\n",
    "* [tor does repacketisation](https://gitlab.torproject.org/tpo/core/torspec/-/blob/main/tor-spec.txt#L488) to obtain a fixed size payload. Therefore we probably **won't find any interesting feature in the payload size**. However, observing the traces on Wireshark, we see that there are different cell sizes (around 500 bytes and 1200 bytes). The repacketization seems therefore to not be absolute and we will still extract the packet size information. \n",
    "* the queries and answers goes through real tor (we can observe the destination ip addresses that are those of real nodes). This yields excessively longer times for getting answers. We have the intuition that the network randomness will prevent an attacker to have meaningful information from the timings of the observed trace. We make a few extra capture and record times to confirm or not our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `tor_time.sh` can be run assuming a runing server and the installation of `bc` (`apt-get install bc`) utility.\n",
    "\n",
    "It basically records the time took to make the query with and without tor for a few grids. We ran it 3 times for the 30 first grids and it shows the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_to_tuple(line: str) -> Tuple[int, float]:\n",
    "    cleaned_line = line.strip().split(\":\")\n",
    "    return (int(cleaned_line[0]), float(cleaned_line[1]))\n",
    "\n",
    "\n",
    "def from_file_to_data(is_with_tor: bool) -> List[Tuple[int, float]]:\n",
    "    times = []\n",
    "    for index in range(1, 5):\n",
    "        with_tor = \"tor_\" if is_with_tor else \"no_tor_\"\n",
    "        filename = \"timings_measures/times_\" + with_tor + str(index) + \".txt\"\n",
    "        times_i = []\n",
    "        with open(filename, \"r\") as file:\n",
    "            times_i = list(map(line_to_tuple, file.readlines()[1:]))\n",
    "        times.append(times_i)\n",
    "    return times\n",
    "\n",
    "\n",
    "no_tor_times = from_file_to_data(False)\n",
    "tor_times = from_file_to_data(True)\n",
    "assert all(len(times) == 30 for times in no_tor_times) and all(\n",
    "    len(times) == 30 for times in tor_times\n",
    ")\n",
    "data = {\n",
    "    \"grid_id\": list(map(lambda t: t[0], no_tor_times[0])),\n",
    "    \"no_tor_time_1\": list(map(lambda t: t[1], no_tor_times[0])),\n",
    "    \"tor_time_1\": list(map(lambda t: t[1], tor_times[0])),\n",
    "    \"no_tor_time_2\": list(map(lambda t: t[1], no_tor_times[1])),\n",
    "    \"tor_time_2\": list(map(lambda t: t[1], tor_times[1])),\n",
    "    \"no_tor_time_3\": list(map(lambda t: t[1], no_tor_times[2])),\n",
    "    \"tor_time_3\": list(map(lambda t: t[1], tor_times[2])),\n",
    "    \"no_tor_time_4\": list(map(lambda t: t[1], no_tor_times[3])),\n",
    "    \"tor_time_4\": list(map(lambda t: t[1], tor_times[3])),\n",
    "}\n",
    "times = pd.DataFrame(data)\n",
    "# Get current axis\n",
    "times.plot(\n",
    "    x=\"grid_id\",\n",
    "    y=[\"no_tor_time_1\", \"no_tor_time_2\", \"no_tor_time_3\", \"no_tor_time_4\"],\n",
    "    title=\"Grid query time without tor over 3 captures (seconds)\",\n",
    "    figsize=(11, 4),\n",
    "    kind=\"line\",\n",
    ")\n",
    "plt.show()\n",
    "times.plot(\n",
    "    x=\"grid_id\",\n",
    "    y=[\"tor_time_1\", \"tor_time_2\", \"tor_time_3\", \"tor_time_4\"],\n",
    "    title=\"Grid query time with tor over 3 captures (seconds)\",\n",
    "    figsize=(11, 4),\n",
    "    kind=\"line\",\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we observe that the pikes between the tor and without tor timings are not exactly coherent, we cannot deny that some correlation happens between the tor observed traces for the same grid id queries.\n",
    "\n",
    "We do not pretend to have reached statistical relevance but the intuition we had about tor killing any pattern of the returned trace does not clearly hold and we'll prefer to continue our study taking it int account.\n",
    "\n",
    "This takes us to the data-science experience \"our theory was wrong\" and we'll therefore **keep the timestamps as useful values for the packet representation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Packet = Tuple[int, float]\n",
    "\"\"\"\n",
    "represents a packet by its:\n",
    "- IN/OUT nature (IN == to client == -1) (OUT == to server == 1) multiplied by the packet size\n",
    "- relative timestamp since the begining of the request for the queried grid\n",
    "\"\"\"\n",
    "\n",
    "dummy = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Received data\n",
    "First, the data we receive is in the form of `.pcap` files. Those were filtered to contain only (see `capture.sh`):\n",
    "* **TCP packets**, other protocols packets are considered not relevant to the application (ARP requests, DNS stuff etc...)\n",
    "* **packets that have a TCP payload**. Basically all TCP ACKs and other controls packets are filtered and not considered as those depend on the networking related traffic and will noise the core of the application layer generated fingerprint\n",
    "\n",
    "From those packets, can be retrieved:\n",
    "* **Ethernet header** (14 bytes, bytes 0 to 13). Those will probably not be relevant for us as depending on the inner Docker networking (or any used LAN environment)\n",
    "* **IP headers** (20 bytes, bytes 14 to 33). Those bytes mostly contain the routing information (source IP, destination IP, routing flags, etc...). The main information we may want to retrieve from this is the direction of the packets we observe (query or response).\n",
    "* **TCP headers** (20 bytes, bytes 34 to 53). These bytes contain the stream informations (sequence and ACK indexes, timestamps, payload size, etc...).\n",
    "* **TLS headers** (10 bytes, bytes 54 to 63). These bytes contains some informations about the version and inner data. the bytes 59 to 63 are the first bytes of the encrypted data which is not recorded as it is not meaningful for us. Indeed the headers already contain the length meta-data and we will not get much more from the correctly encrypted payload (unless we make cryptographic attacks which does not seem reasonnable as the TLS1.2 version is used here).\n",
    "\n",
    "From this, we are able to define the data we'll want to keep and output to our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desired data model\n",
    "First, we must decide on the way we'll represent our data as features. As seen before, not all the data will be useful! Our main goal will be to capture in our features the data that contains the most variance between samples of different labels.\n",
    "\n",
    "The ideal representation would be storing the full trace as our features.\n",
    "Label (grid_id queried) | Trace (packet list generated)\n",
    ":---: | :---: \n",
    "1 | `[pkt1,pkt2,...]`\n",
    "\n",
    "However, before burning down our computer, we must wonder what trace size it is reasonnable to hold in memory. Holding the whole dataset in memory may require using swap memory and this is what brought an end to my previous laptop.\n",
    "Let us make some calculations to see if it is reasonnable to hold the 100 grid traces (namely, the full trace associated to an index as described in our TraceTuple):\n",
    "* let us assume a trace is $1000$ packet (upperbound we observe).\n",
    "* Python int and float are 28 and 24 bytes, meaning the size of a `Packet` type tuple is $28+24=52$ bytes (see below python cell)\n",
    "* there are $100$ traces for a capture index\n",
    "This gives us a total of $1000*52*100=5.2$ MB \n",
    "\n",
    "As we may want to potentially hold tens or hundreds of such traces in order to perform our classification, it seems we'll clearly need a more compressed feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.getsizeof(int(-1)))\n",
    "print(sys.getsizeof(float(time.time())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the client and server code, we notice that the main things that may vary between 2 traces are:\n",
    "* The number of PoI returned\n",
    "* The number of ratings per PoI\n",
    "\n",
    "Closer look at the trace, we observe that the communication happens in \"rounds\": indeed, the client first retrieves the list of PoI and then, queries the list of ratings for each PoI successively. Identifying the number, size and direction of each round, we can be pretty confident that we'll keep the most of the fingerprint information of this communication. Also, as we remarked correlation between exchange time and label, we also put this metric as a feature. We therefore can decide on representing our features as:\n",
    "Label (grid_id queried) | Nb of exchanged packets | Exchange time (s) | Nb of rounds (should be linear with nb of PoI) | Size round 1 | Time round 1 | Size round 2 | Time round 2 | ... | Size round N (max nb of round observed) | Time round N\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---:| :---:\n",
    "e.g.: 1 | 306 | 23.15721 | 12 | 3 | 2.19813 | -5 | 6.2194 | ... | 0 | 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = dict[str, Any]\n",
    "\"\"\"Represents the features vector as defined above\"\"\"\n",
    "dummy = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction from pcap file\n",
    "### Data cleaning\n",
    "We first take a look at the the retrieved traces. We will see if some badly saved traces are present and decide if they should be removed.\n",
    "\n",
    "To do so, we have defined the `TraceTuple` type that identifies a trace. It also contains the length of this trace, this will be our main measure of healthiness of the capture: we expect our traces to differ but would like to detect huge outliers that might come from a networking issue during the capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TraceTuple = Tuple[str, int, int, int]\n",
    "\"\"\"\n",
    "stores the trace name along with its informations for data cleaning:\n",
    "- trace name (the filename)\n",
    "- trace index (identifies traces with same date-hour)\n",
    "- trace grid ID (the id of the queried grid)\n",
    "- trace length\n",
    "\"\"\"\n",
    "dummy = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_file_to_pkt_count(file: str, root: str = \"traces\") -> int:\n",
    "    \"\"\"Memory efficient function to return the number of packets in a pcap file without loading the full file in memory. Also makes sure we can iterate through all packets of a trace\"\"\"\n",
    "    size = 0\n",
    "    # if the pcap file is corrupted, we should have an error in the file reading there\n",
    "    with PcapReader(\"{}/{}\".format(root, file)) as pcap:\n",
    "        for pkt in pcap:\n",
    "            size += 1\n",
    "    return size\n",
    "\n",
    "\n",
    "# exemple\n",
    "from_file_to_pkt_count(\"trace_01_04_14h_grid_69.pcap\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_file_to_trace_tuple(file: str, root: str = \"traces\") -> TraceTuple:\n",
    "    \"\"\"Allows us to read a file and produce our desired TraceTuple\"\"\"\n",
    "    if not file.endswith(\".pcap\"):\n",
    "        raise ValueError(\"give only .pcap files as input\")\n",
    "    trace_name = file.removesuffix(\".pcap\").split(\"_\")\n",
    "    # allow us to uniquely index uniquely any trace by an integer\n",
    "    trace_index = (\n",
    "        int(trace_name[1]) * 10000\n",
    "        + int(trace_name[2]) * 100\n",
    "        + int(trace_name[3].removesuffix(\"h\"))\n",
    "    )\n",
    "    grid_index = int(trace_name[5])\n",
    "    return file, trace_index, grid_index, from_file_to_pkt_count(file, root=root)\n",
    "\n",
    "\n",
    "# exemple\n",
    "trace_tuple = from_file_to_trace_tuple(\"trace_01_04_08h_grid_1.pcap\")\n",
    "trace_tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_traces() -> List[TraceTuple]:\n",
    "    \"\"\"Returns a list of tuple describing all traces\"\"\"\n",
    "    for root, dirs, files in os.walk(\"traces\"):\n",
    "        pcap_files = list(filter(lambda f: f.endswith(\".pcap\"), files))\n",
    "        print(\"{} traces in the folder\".format(len(pcap_files)))\n",
    "        # we use eventual parallelization to read faster\n",
    "        with Pool() as p:\n",
    "            return p.map(from_file_to_trace_tuple, pcap_files)\n",
    "\n",
    "\n",
    "all_traces = read_all_traces()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a pandas `DataFrame` to store our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"name\": list(map(lambda t: t[0], all_traces)),\n",
    "    \"index\": list(map(lambda t: t[1], all_traces)),\n",
    "    \"grid_id\": list(map(lambda t: t[2], all_traces)),\n",
    "    \"packet_list_len\": list(map(lambda t: t[3], all_traces)),\n",
    "}\n",
    "traces = pd.DataFrame(data)\n",
    "traces.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at the basic statistics of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces.groupby(\"grid_id\").describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to make sure that no capture issue occured. The following boxplot allow us to see the distribution of the packet list length for the traces grouped by the grid_id that generated the trace.\n",
    "\n",
    "We expect that traces generated by same grid_id can differ but remain in approximatley same order of magnitude: indeed, observing the client code, we see that the added noise that extends the length of the returned information for a PoI is a random up to a `noise_factor` of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 4))\n",
    "strip = sns.stripplot(x=traces[\"grid_id\"], y=traces[\"packet_list_len\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to identify where do the observed outliers come from, we compute the Z-Score of the column. Basically, center and normalize our column's values and will identify the values that differ from the mean of more than 3 standard-deviations ($z>3$) which is a standard measure to detect huge outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(traces[\"packet_list_len\"]))\n",
    "outliers = traces[z > 3]\n",
    "display(outliers)\n",
    "filtered = traces[z < 3]\n",
    "plt.figure(figsize=(17, 4))\n",
    "strip = sns.stripplot(x=filtered[\"grid_id\"], y=filtered[\"packet_list_len\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe these outliers are noisy for the meaningfulness of our capture and therefore remove them from the set. Those outliers not being all on the same grid query, we are pretty confident that they come from random capture error. We therefore ban them and take care of making sure we do not reduce too much the signal given on the cpature for some labels (if all fails would have been on the same grid query, we should have worried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_traces = filtered.copy()  # traces[~traces[\"index\"].isin(outliers[\"index\"])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction\n",
    "Now we are pretty confident we kept useful data only, we can therefore go ahead and extract the packets as defined by `Packet` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_packet_timestamp(trace_name: str, root: str = \"traces\") -> float:\n",
    "    \"\"\"Extracts the timestamp from the first packet of a trace\"\"\"\n",
    "    timestamp = None\n",
    "    with PcapReader(\"{}/{}\".format(root, trace_name)) as pcap:\n",
    "        timestamp = pcap.read_packet().time\n",
    "    return float(timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pcap_pkt_to_packet(pcap_pkt: scapy.packet.Packet, start: float) -> Packet:\n",
    "    \"\"\"Extract the desired informations from a scapy packet to return the desired tuple to represent a packet\"\"\"\n",
    "    # as explained in the Packet type, timestamps are relative to the begining of the trace\n",
    "    in_out, rel_timestamp = None, None\n",
    "    if pcap_pkt.haslayer(IP):\n",
    "        # we use a trick here: if the ip source is private, the packet is **coming from the client** (as the observer is between client and tor entry node). Every client is initialized with a private IP address as running on our laptops\n",
    "        in_out = 1 if ipaddress.ip_address(pcap_pkt.getlayer(IP).src).is_private else -1\n",
    "        # we ponderate by the number of bytes that are sent (the packet size)\n",
    "        in_out *= pcap_pkt[IP].len\n",
    "    else:\n",
    "        raise ValueError(\"Packet without ip field\")\n",
    "    # due to floating point comparaison, we take care of the first packet timestamp value\n",
    "    rel_timestamp = (\n",
    "        0.0\n",
    "        if float(pcap_pkt.time) - start < 10 ** (-7)\n",
    "        else float(pcap_pkt.time) - start\n",
    "    )\n",
    "    return in_out, rel_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_trace_to_packets(trace_name: str, root: str = \"traces\") -> List[Packet]:\n",
    "    \"\"\"Gives the list of Packet type objects contained in the trace\"\"\"\n",
    "    start = get_first_packet_timestamp(trace_name, root)\n",
    "    pkt_list = []\n",
    "    with PcapReader(\"{}/{}\".format(root, trace_name)) as pcap:\n",
    "        for pkt in pcap:\n",
    "            pkt_list.append(from_pcap_pkt_to_packet(pkt, start))\n",
    "    return pkt_list\n",
    "\n",
    "\n",
    "packets = from_trace_to_packets(\"trace_01_04_08h_grid_4.pcap\")\n",
    "packets[:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(n: int) -> int:\n",
    "    return 1 if n > 0 else -1 if n < 0 else 0\n",
    "\n",
    "\n",
    "def from_packet_list_to_rounds(\n",
    "    packets: List[Packet], max_nb_rounds: int = 0\n",
    ") -> Tuple[int, List[int]]:\n",
    "    round_sizes = []\n",
    "    round_timings = []\n",
    "    current_direction = sign(packets[0][0])\n",
    "    current_size = 0\n",
    "    current_time = 0.0\n",
    "    for pkt in packets:\n",
    "        if sign(pkt[0]) != current_direction:\n",
    "            # appends +1*round size if outgoing round or -1*round size if incomming round\n",
    "            round_sizes.append(current_direction * current_size)\n",
    "            round_timings.append(current_time)\n",
    "            current_size = 0\n",
    "        current_size += abs(pkt[0])\n",
    "        current_direction = sign(pkt[0])\n",
    "        current_time = pkt[1]\n",
    "    return len(round_sizes), list(zip(round_sizes, round_timings)) + [\n",
    "        (0, 0.0),\n",
    "    ] * (max_nb_rounds - len(round_sizes))\n",
    "\n",
    "\n",
    "ex_nb_rounds, ex_rounds = from_packet_list_to_rounds(\n",
    "    from_trace_to_packets(\"trace_01_04_08h_grid_4.pcap\")\n",
    ")\n",
    "ex_nb_rounds, ex_rounds[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, we want to have a way to know the upperbound of the number of rounds we may observe. This one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_trace_to_nb_rounds(trace_name: str, root: str = \"traces\") -> int:\n",
    "    packets = from_trace_to_packets(trace_name, root)\n",
    "    nb_rounds, _ = from_packet_list_to_rounds(packets)\n",
    "    return nb_rounds\n",
    "\n",
    "\n",
    "def get_max_nb_rounds(traces: DataFrame, root: str = \"traces\") -> int:\n",
    "    with Pool() as p:\n",
    "        return max(p.map(from_trace_to_nb_rounds, list(traces[\"name\"].unique())))\n",
    "\n",
    "\n",
    "MAX_NB_ROUNDS = get_max_nb_rounds(selected_traces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_trace_to_feature(\n",
    "    trace_name: str, max_rounds: int, root: str = \"traces\"\n",
    ") -> Features:\n",
    "    trace_infos = from_file_to_trace_tuple(trace_name, root)\n",
    "    packets = from_trace_to_packets(trace_name, root)\n",
    "    feature = dict()\n",
    "    feature[\"label\"] = [trace_infos[2]]\n",
    "    feature[\"size\"] = [len(packets)]\n",
    "    feature[\"time\"] = [packets[-1][1]]\n",
    "    nb_rounds, rounds = from_packet_list_to_rounds(packets, max_rounds)\n",
    "    feature[\"nb_rounds\"] = [nb_rounds]\n",
    "    for i, round in enumerate(rounds):\n",
    "        feature[\"round{}_size\".format(i + 1)] = [round[0]]\n",
    "        feature[\"time_round{}\".format(i + 1)] = [round[1]]\n",
    "    return feature\n",
    "\n",
    "\n",
    "list(from_trace_to_feature(\"trace_01_04_08h_grid_4.pcap\", MAX_NB_ROUNDS).items())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_traces(\n",
    "    index: int,\n",
    "    traces: DataFrame = selected_traces,\n",
    "    max_rounds: int = MAX_NB_ROUNDS,\n",
    "    root: str = \"traces\",\n",
    ") -> DataFrame:\n",
    "    all_features = dict()\n",
    "    for trace_name in traces[traces[\"index\"] == index][\"name\"]:\n",
    "        features = from_trace_to_feature(trace_name, max_rounds, root)\n",
    "        for ft in features.keys():\n",
    "            if not ft in all_features:\n",
    "                all_features[ft] = []\n",
    "            all_features[ft] += features[ft]\n",
    "    return pd.DataFrame(all_features)\n",
    "\n",
    "\n",
    "features_df = get_features_for_traces(10408, selected_traces, MAX_NB_ROUNDS)\n",
    "features_df.info()\n",
    "display(features_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_dataframe() -> DataFrame:\n",
    "    with Pool() as p:\n",
    "        data_frames = p.map(\n",
    "            get_features_for_traces, list(selected_traces[\"index\"].unique())\n",
    "        )\n",
    "        return reduce(lambda x, y: pd.concat([x, y]), data_frames)\n",
    "\n",
    "\n",
    "full_df = create_full_dataframe()\n",
    "full_df.info()\n",
    "display(full_df)\n",
    "full_df.to_csv(\"features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array = pd.read_csv(\"features.csv\").filter(regex=\"label|round.*_size\")\n",
    "feature_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array.transpose()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_array.transpose()[1:].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
